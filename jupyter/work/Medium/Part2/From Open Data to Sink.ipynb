{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Neo4j Streams Sink\n",
    "\n",
    "This module allows Neo4j consuming data from a Kafka topic, and it does it in a \"smart\" way, by allowing you to define your custom queries. What you need to do is write in your neo4j.conf something like this:\n",
    "\n",
    "`streams.sink.topic.cypher.<TOPIC>=<CYPHER_QUERY>`\n",
    "\n",
    "So if you define a query just like this:\n",
    "\n",
    "```\n",
    "streams.sink.topic.my-topic=MERGE (n:Person{id: event.id}) \\\n",
    "    ON CREATE SET n += event.properties\n",
    "```\n",
    "\n",
    "And for events like this:\n",
    "\n",
    "`{id:\"alice@example.com\",properties:{name:\"Alice\",age:32}}`\n",
    "\n",
    "Under the hood the Sink module will execute a query like this:\n",
    "\n",
    "```\n",
    "UNWIND {batch} AS event\n",
    "MERGE (n:Label {id: event.id})\n",
    "    ON CREATE SET n += event.properties\n",
    "```\n",
    "\n",
    "So continuing with the example above a possible full representation could be:\n",
    "\n",
    "```\n",
    "WITH [{id:\"alice@example.com\",properties:{name:\"Alice\",age:32}},\n",
    "    {id:\"bob@example.com\",properties:{name:\"Bob\",age:42}}] AS batch\n",
    "UNWIND batch AS event\n",
    "MERGE (n:Person {id: event.id})\n",
    "    ON CREATE SET n += event.properties\n",
    "```\n",
    "\n",
    "This gives to the developer the power to define his own business rules because you can choose to update, add to, remove, adapt your graph data based on the events you get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our configuration\n",
    "\n",
    "```\n",
    "NEO4J_streams_sink_topic_cypher_pharma: \"\n",
    "          MERGE (p:Pharmacy{fiscalId: event.FISCAL_ID}) ON CREATE SET p.name = event.NAME\n",
    "          MERGE (t:PharmacyType{type: event.TYPE_NAME})\n",
    "          MERGE (a:Address{name: event.ADDRESS + ', ' + event.CITY})\n",
    "            ON CREATE SET a.latitude = event.LATITUDE, a.longitude = event.LONGITUDE,\n",
    "              a.code = event.POSTAL_CODE, a.point = event.POINT\n",
    "          MERGE (c:City{name: event.CITY})\n",
    "          MERGE (p)-[:IS_TYPE]-(t)\n",
    "          MERGE (p)-[:HAS_ADDRESS]-(a)\n",
    "          MERGE (a)-[:IS_LOCATED_IN]->(c)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# (Neo4j)-[:LOVES]->(Kafka)\n",
    "\n",
    "## Architecture\n",
    "\n",
    "<div class=\"img-responsive center-block\" style=\"background-image: url('https://cdn-images-1.medium.com/max/2000/1*0RNrK1OSS779TJ6F3sysjQ.png'); width: 1327px; height: 300px; background-position: center; background-size: cover;\"></div>\n",
    "\n",
    "# The Open Data API\n",
    "\n",
    "We'll use the Italian Ministry of Health dataset of Pharmacy stores.\n",
    "\n",
    "## The data model\n",
    "\n",
    "<div class=\"img-responsive center-block\" style=\"background-image: url('https://cdn-images-1.medium.com/max/1600/1*1J4GGP2XenkCfuBi8ZCrjw.png'); width: 481px; height: 134px; background-position: center; background-size: cover;\"></div>\n",
    "\n",
    "## Link\n",
    "\n",
    "https://neo4j-contrib.github.io/neo4j-streams/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark & Neo4j Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local').config(\n",
    "    \"spark.jars.packages\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\"\n",
    ").getOrCreate()\n",
    "\n",
    "# Neo4j\n",
    "import sys\n",
    "!{sys.executable} -m pip install py2neo sparksql-magic neographviz\n",
    "\n",
    "from py2neo import Graph\n",
    "\n",
    "graph = Graph(\"bolt://neo4j:7687\", auth=(\"neo4j\", \"zeppelin\"))\n",
    "\n",
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the url of the Open Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileUrl = input(\"File URL: \") # http://www.dati.salute.gov.it/imgs/C_17_dataset_5_download_itemDownload0_upFile.CSV\n",
    "\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "fileUrlParsed = urlparse(fileUrl)\n",
    "localFilePath = \"/home/streams/\" + os.path.basename(fileUrlParsed.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Open Data dataset from the URL (if it's not already present in the file system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(fileUrl, allow_redirects=True) \n",
    "with open(localFilePath, 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CSV into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(localFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF.createOrReplaceTempView(\"open_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the Dataframe and replace the old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW OPEN_DATA_EN AS\n",
    "SELECT CODICEIDENTIFICATIVOFARMACIA AS PHARMA_ID,\n",
    "    INDIRIZZO AS ADDRESS,\n",
    "    DESCRIZIONEFARMACIA AS NAME,\n",
    "    PARTITAIVA AS FISCAL_ID,\n",
    "    CAP AS POSTAL_CODE,\n",
    "    DESCRIZIONECOMUNE AS CITY,\n",
    "    DESCRIZIONETIPOLOGIA AS TYPE_NAME,\n",
    "    REGEXP_REPLACE(LATITUDINE, ',', '.') AS LATITUDE,\n",
    "    REGEXP_REPLACE(LONGITUDINE, ',', '.') AS LONGITUDE,\n",
    "    CONCAT(REGEXP_REPLACE(LATITUDINE, ',', '.'), ',', REGEXP_REPLACE(LONGITUDINE, ',', '.')) AS POINT\n",
    "FROM OPEN_DATA\n",
    "WHERE DATAFINEVALIDITA <> '-'\n",
    "AND CODICEIDENTIFICATIVOFARMACIA <> '-'\n",
    "AND PARTITAIVA <> '-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new Dataframe that will be used to send the data to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW OPEN_DATA_KAFKA_STAGE AS\n",
    "SELECT \n",
    "    PHARMA_ID AS KEY,\n",
    "    TO_JSON(\n",
    "        STRUCT(PHARMA_ID,\n",
    "            ADDRESS,\n",
    "            NAME,\n",
    "            FISCAL_ID,\n",
    "            POSTAL_CODE,\n",
    "            CITY,\n",
    "            TYPE_NAME,\n",
    "            LATITUDE,\n",
    "            LONGITUDE,\n",
    "            POINT)\n",
    "    ) AS VALUE\n",
    "FROM OPEN_DATA_EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "\n",
    "select * from OPEN_DATA_KAFKA_STAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Constrains on Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"CREATE CONSTRAINT ON (p:Pharmacy) ASSERT p.fiscalId IS UNIQUE\")\n",
    "graph.run(\"CREATE CONSTRAINT ON (t:PharmacyType) ASSERT t.type IS UNIQUE\")\n",
    "graph.run(\"CREATE CONSTRAINT ON (a:Address) ASSERT a.name IS UNIQUE\")\n",
    "graph.run(\"CREATE CONSTRAINT ON (c:City) ASSERT c.name IS UNIQUE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the DB (if it's necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"MATCH (n) DETACH DELETE n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the current content of the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"MATCH (n) return count(n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the staging Dataset and send the data to the \"pharma\" topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"OPEN_DATA_KAFKA_STAGE\") \\\n",
    "\t.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    \t.write \\\n",
    "    \t.format(\"kafka\") \\\n",
    "    \t.option(\"kafka.enable.auto.commit\", \"true\") \\\n",
    "    \t.option(\"kafka.bootstrap.servers\", \"broker:9093\") \\\n",
    "    \t.option(\"topic\", \"pharma\") \\\n",
    "    \t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the dataset over Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = input(\"City: \")\n",
    "query = \"MATCH x=(p:Pharmacy)-[pha:HAS_ADDRESS]->(a:Address)-[aic:IS_LOCATED_IN]->(c:City{{name:'{city}'}}) RETURN x LIMIT 100\".format(**locals())\n",
    "\n",
    "from neographviz import Graph, plot\n",
    "\n",
    "plot(graph, query)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}