{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging the Neo4j Streams module - Part 1 - Build a Just-In-Time Datawarehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Legacy Structure\n",
    "\n",
    "## Enterprise Data Warehouses\n",
    "Traditional DWH requires data teams to constantly build multiple costly and time-consuming **E**xtract **T**ransform **L**oad (ETL) pipelines to ultimately derive business insights.\n",
    "One of the biggest pain points is that, due to its **Rigid architecture that's difficult to chang*e*, Enterprise Data Warehouses are **inherently rigid**. That's because:\n",
    "\n",
    "* they are **based on** the **Schema-On-Write** architecture: first, you define your schema, then you write your data, then you read your data and it comes back in the schema you defined up-front;\n",
    "* they are **based on** (expensive) **batched/scheduled jobs**;\n",
    "\n",
    "**This results in having to build costly and time-consuming ETL pipeline** to access and manipulate the data. And as **new data types** and sources are introduced, the need to augment your ETL pipelines **exacerbates the problem**.\n",
    "Thanks to the combination of the stream data processing with the Neo4j Streams CDC module and the Schema-On-Read approach provided by Apache Spark we can overcome this rigidity and build a new kind of (flexible) DWH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A paradigm shift: Just-In-Time Data Warehouse\n",
    "\n",
    "A JIT-DWH solution is designed to easily handle a wider variety of data from different sources an start from a different approach about how to deal and manage data: **Schema-On-Read**\n",
    "\n",
    "## Schema-On-Read\n",
    "\n",
    "Schema-On-Read follows a different sequence, it just loads the data as-is and applies your own lens to the data when you read it back out. With this kind of approach you can present data in a schema that is adapted best to the queries being issued. You're not stuck with a one-size-fits-all schema. With schema-on-read, you can present the data back in a schema that is most relevant to the task at hand.\n",
    "\n",
    "## How to?\n",
    "\n",
    "Apply this kind of schema is pretty simple and leverage the **Neo4j Streams** module with **Apache Spark**'s Structured Streaming Apis and **Apache Kafka**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging the Neo4j Streams Change Data Capture (CDC)\n",
    "\n",
    "The Neo4j Streams project is composed of three main pillars:\n",
    "\n",
    "* The Change Data Capture that allows to stream database changes over Kafka topics\n",
    "* The Sink that allows consuming data streams from Kafka topic\n",
    "* A set of procedures that allows to Produce/Consume data to/from Kafka Topics\n",
    "\n",
    "This notebook is focused on the Change Data Capture\n",
    "\n",
    "## What is a Change Data Capture?\n",
    "\n",
    "It's a system that automatically captures changes from a source system (a Datatabase for instace) and automatically notifies these changes to a target system.\n",
    "CDC typically forms part of an ETL pipeline. This is an important component for ensuring Data Warehouses (DWH) are kept up to date with any record changes.\n",
    "Also traditionally CDC applications worked off of transaction logs, thereby allowing to replicate databases without having a/much performance impact on its operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Neo4j Streams CDC module deal with database changes?\n",
    "\n",
    "Every transaction inside Neo4j gets intercept and unpacked in order to stream an atomic element of the transaction.\n",
    "Let's suppose we have a simple creation of two nodes and one relationship between them:\n",
    "\n",
    "```\n",
    "CREATE (andrea:Person{name:\"Andrea\"})-[knows:KNOWS{since:2014}]->(michael:Person{name:\"Michael\"})\n",
    "```\n",
    "\n",
    "The CDC module will unpack this transaction into 3 events (2 node creation, 1 relationship creation). And the data streamed from the CDC has the following structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Event structure was inspired by the [Debezium](https://debezium.io/) format and has the following general structure:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"meta\": { /* transaction meta-data */ },\n",
    "  \"payload\": { /* the data related to the transaction */\n",
    "    \"before\": { /* the data before the transaction */},\n",
    "    \"after\": { /* the data after the transaction */}\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node source `(andrea)`:\n",
    "```\n",
    "{\n",
    "  \"meta\": {\n",
    "    \"timestamp\": 1532597182604,\n",
    "    \"username\": \"neo4j\",\n",
    "    \"tx_id\": 1,\n",
    "    \"tx_event_id\": 0,\n",
    "    \"tx_events_count\": 3,\n",
    "    \"operation\": \"created\",\n",
    "    \"source\": {\n",
    "      \"hostname\": \"neo4j.mycompany.com\"\n",
    "    }\n",
    "  },\n",
    "  \"payload\": {\n",
    "    \"id\": \"1004\",\n",
    "    \"type\": \"node\",\n",
    "    \"after\": {\n",
    "      \"labels\": [\"Person\"],\n",
    "      \"properties\": {\n",
    "        \"name\": \"Andrea\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Node target `(michael)`:\n",
    "```\n",
    "{\n",
    "  \"meta\": {\n",
    "    \"timestamp\": 1532597182604,\n",
    "    \"username\": \"neo4j\",\n",
    "    \"tx_id\": 1,\n",
    "    \"tx_event_id\": 1,\n",
    "    \"tx_events_count\": 3,\n",
    "    \"operation\": \"created\",\n",
    "    \"source\": {\n",
    "      \"hostname\": \"neo4j.mycompany.com\"\n",
    "    }\n",
    "  },\n",
    "  \"payload\": {\n",
    "    \"id\": \"1006\",\n",
    "    \"type\": \"node\",\n",
    "    \"after\": {\n",
    "      \"labels\": [\"Person\"],\n",
    "      \"properties\": {\n",
    "        \"name\": \"Michael\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Relationship `knows`:\n",
    "```\n",
    "{\n",
    "  \"meta\": {\n",
    "    \"timestamp\": 1532597182604,\n",
    "    \"username\": \"neo4j\",\n",
    "    \"tx_id\": 1,\n",
    "    \"tx_event_id\": 2,\n",
    "    \"tx_events_count\": 3,\n",
    "    \"operation\": \"created\",\n",
    "    \"source\": {\n",
    "      \"hostname\": \"neo4j.mycompany.com\"\n",
    "    }\n",
    "  },\n",
    "  \"payload\": {\n",
    "    \"id\": \"1007\",\n",
    "    \"type\": \"relationship\",\n",
    "    \"label\": \"KNOWS\",\n",
    "    \"start\": {\n",
    "      \"labels\": [\"Person\"],\n",
    "      \"id\": \"1005\"\n",
    "    },\n",
    "    \"end\": {\n",
    "      \"labels\": [\"Person\"],\n",
    "      \"id\": \"106\"\n",
    "    },\n",
    "    \"after\": {\n",
    "      \"properties\": {\n",
    "        \"since\": 2014\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To a more in-depth description of the Neo4j Streams project and how/why we at [LARUS](http://www.larus-ba.it/) and [Neo4j](https://neo4j.com/) built it, check out [this article]((https://medium.com/neo4j/a-new-neo4j-integration-with-apache-kafka-6099c14851d2)) that provides an in-depth description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Spark & Neo4j Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local').config(\n",
    "    \"spark.jars.packages\",\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\"\n",
    ").getOrCreate()\n",
    "\n",
    "# Neo4j\n",
    "import sys\n",
    "!{sys.executable} -m pip install py2neo\n",
    "\n",
    "from py2neo import Graph\n",
    "\n",
    "graph = Graph(\"bolt://neo4j:7687\", auth=(\"neo4j\", \"streams\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create random Social Network Data\n",
    "\n",
    "We'll create a fake social network by using the APOC apoc.periodic.repeat procedure that executes every 15 seconds this query:\n",
    "\n",
    "```\n",
    "WITH [\"M\", \"F\", \"\"] AS gender\n",
    "UNWIND range(1, 10) AS id\n",
    "CREATE (p:Person {id: apoc.create.uuid(), name: \"Name-\" +  apoc.text.random(10), age: round(rand() * 100), index: id, gender: gender[toInteger(size(gender) * rand())]})\n",
    "WITH collect(p) AS people\n",
    "UNWIND people AS p1\n",
    "UNWIND range(1, 3) AS friend\n",
    "WITH p1, people[(p1.index + friend) % size(people)] AS p2\n",
    "CREATE (p1)-[:KNOWS{years: round(rand() * 10), engaged: (rand() > 0.5)}]->(p2)\n",
    "```\n",
    "\n",
    "If you need more details about how the APOC project please follow this [link](https://neo4j-contrib.github.io/neo4j-apoc-procedures/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute this query if you want to clean-up the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"MATCH (n) DETACH DELETE n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an index on Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"CREATE INDEX ON :Person(id)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch background job for populating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL apoc.periodic.repeat(\n",
    "  'create-fake-social-data', \n",
    "  '\n",
    "     WITH [\"M\", \"F\", \"X\"] AS gender \n",
    "     UNWIND range(1, 10) AS id \n",
    "     CREATE (p:Person {\n",
    "       id: apoc.create.uuid(), \n",
    "       name: \"Name-\" +  apoc.text.random(10), \n",
    "       age: round(rand() * 100), \n",
    "       index: id, \n",
    "       gender: gender[toInteger(size(gender) * rand())]})\n",
    "     WITH collect(p) AS people\n",
    "     UNWIND people AS p1\n",
    "     UNWIND range(1, 3) AS friend\n",
    "     WITH p1, people[(p1.index + friend) % size(people)] AS p2\n",
    "     CREATE (p1)-[:KNOWS{years: round(rand() * 10), engaged: (rand() > 0.5)}]->(p2)\n",
    "  ', 15) YIELD name\n",
    "RETURN name AS created\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this query when you want to stop the background data population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"CALL apoc.periodic.cancel('create-fake-social-data') YIELD name RETURN name AS cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how our data grow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"MATCH p = (p1:Person)-[k:KNOWS]->(p2:Person) RETURN count(p)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Structured Streaming Dataset that consumes the data from a \"neo4j\" topic (the default topic of the CDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaStreamingDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:9093\") \\\n",
    "    .option(\"startingoffsets\", \"earliest\") \\\n",
    "    .option(\"subscribe\", \"neo4j\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The structure of the data, as you can see is basically a Kafka ProducerRecord representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaStreamingDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the Structure of the data streamed by the CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, MapType, ArrayType, StringType, LongType\n",
    "\n",
    "cdcMetaSchema = StructType([\n",
    "\tStructField(\"timestamp\", LongType()),\n",
    "\tStructField(\"username\", StringType()),\n",
    "\tStructField(\"operation\", StringType()),\n",
    "\tStructField(\"source\", MapType(StringType(), StringType(), True))\n",
    "])\n",
    "    \n",
    "cdcPayloadSchemaBeforeAfter = StructType([\n",
    "\tStructField(\"labels\", ArrayType(StringType(), False)),\n",
    "\tStructField(\"properties\", MapType(StringType(), StringType(), True))\n",
    "])\n",
    "    \n",
    "cdcPayloadSchema = StructType([\n",
    "\tStructField(\"id\", StringType()),\n",
    "\tStructField(\"type\", StringType()),\n",
    "\tStructField(\"label\", StringType()),\n",
    "\tStructField(\"start\", MapType(StringType(), StringType(), True)),\n",
    "\tStructField(\"end\", MapType(StringType(), StringType(), True)),\n",
    "\tStructField(\"before\", cdcPayloadSchemaBeforeAfter),\n",
    "\tStructField(\"after\", cdcPayloadSchemaBeforeAfter)\n",
    "])\n",
    "    \n",
    "cdcSchema = StructType([\n",
    "\tStructField(\"meta\", cdcMetaSchema),\n",
    "\tStructField(\"payload\", cdcPayloadSchema)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's extract only the CDC Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "cdcDataFrame = kafkaStreamingDF \\\n",
    "    .selectExpr(\"CAST(value AS STRING) AS VALUE\") \\\n",
    "    .select(from_json('VALUE', cdcSchema).alias('JSON'))\n",
    "\n",
    "cdcDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's perform a simple ETL query in order to extract fields of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataWarehouseDataFrame = cdcDataFrame \\\n",
    "    .where(\"json.payload.type = 'node' and (array_contains(nvl(json.payload.after.labels, json.payload.before.labels), 'Person'))\") \\\n",
    "    .selectExpr(\"json.payload.id AS neo_id\", \"CAST(json.meta.timestamp / 1000 AS Timestamp) AS timestamp\", \\\n",
    "        \"json.meta.source.hostname AS host\", \\\n",
    "        \"json.meta.operation AS operation\", \\\n",
    "        \"nvl(json.payload.after.labels, json.payload.before.labels) AS labels\", \\\n",
    "        \"explode(json.payload.after.properties)\")\n",
    "dataWarehouseDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you want debug your streaming query please execute this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dataWarehouseDataFrame \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"debug\") \\\n",
    "    .start()\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "timeout = 10\n",
    "timeout_start = time.time()\n",
    "\n",
    "while time.time() < timeout_start + timeout:\n",
    "    clear_output(wait=True)\n",
    "    display(spark.sql('SELECT * FROM debug').show())\n",
    "    time.sleep(2)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data streamed from the Neo4j CDC over the Filesystem as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeOnDisk = dataWarehouseDataFrame \\\n",
    "    .writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/streams/jit-dwh/checkpoint\") \\\n",
    "    .option(\"path\", \"/home/streams/jit-dwh\") \\\n",
    "    .queryName(\"nodes\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch this paragraph if you want to stop the streaming job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in spark.streams.active:\n",
    "\tx.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}